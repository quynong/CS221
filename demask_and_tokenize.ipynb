{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demask, Tokenize và Gán Labels cho PII Dataset\n",
    "\n",
    "Notebook này thực hiện:\n",
    "1. **Demask**: Thay thế PII placeholders bằng giá trị thực từ danh sách tiếng Việt\n",
    "2. **Tokenize**: Sử dụng mBERT tokenizer để tokenize text\n",
    "3. **Gán Labels**: Tạo BIO labels dựa trên các entity được đánh dấu bằng XML tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import thư viện\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load mBERT Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Load tokenizer từ HuggingFace: bert-base-multilingual-cased\n",
      "Tokenizer loaded! Vocabulary size: 119547\n"
     ]
    }
   ],
   "source": [
    "# Load mBERT tokenizer (ưu tiên từ model local nếu có)\n",
    "model_path = \"pii_mBert_case\"\n",
    "\n",
    "if os.path.exists(model_path) and os.path.exists(os.path.join(model_path, \"tokenizer.json\")):\n",
    "    print(f\"✓ Load tokenizer từ model local: {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "else:\n",
    "    print(\"✓ Load tokenizer từ HuggingFace: bert-base-multilingual-cased\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "print(f\"Tokenizer loaded! Vocabulary size: {len(tokenizer.vocab)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load danh sách PII tiếng Việt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Đã load 57 loại PII từ JSON\n",
      "\n",
      "Các loại PII có sẵn:\n",
      "  - PHONEIMEI: 1707 giá trị\n",
      "  - JOBAREA: 47 giá trị\n",
      "  - FIRSTNAME: 2823 giá trị\n",
      "  - VEHICLEVIN: 819 giá trị\n",
      "  - AGE: 311 giá trị\n",
      "  - GENDER: 2 giá trị\n",
      "  - HEIGHT: 492 giá trị\n",
      "  - BUILDINGNUMBER: 906 giá trị\n",
      "  - MASKEDNUMBER: 1946 giá trị\n",
      "  - PASSWORD: 2352 giá trị\n"
     ]
    }
   ],
   "source": [
    "# Load PII lists từ JSON file\n",
    "pii_json_path = \"data/pii_entities_by_label_vietnamese_v10.json\"\n",
    "\n",
    "with open(pii_json_path, 'r', encoding='utf-8') as f:\n",
    "    pii_data = json.load(f)\n",
    "\n",
    "print(f\"✓ Đã load {len(pii_data)} loại PII từ JSON\")\n",
    "print(f\"\\nCác loại PII có sẵn:\")\n",
    "for label, values in list(pii_data.items())[:10]:\n",
    "    print(f\"  - {label}: {len(values)} giá trị\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load dữ liệu từ CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Đã load 24296 dòng từ CSV\n",
      "\n",
      "Các cột trong file:\n",
      "  - source_text\n",
      "  - target_text\n",
      "  - tokens\n",
      "  - labels\n",
      "\n",
      "⚠ Cột 'target_text_vi' không có trong file!\n",
      "Các cột có sẵn: ['source_text', 'target_text', 'tokens', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file\n",
    "csv_path = r\"D:\\CODE\\PII\\data\\dong10000_32000_data1.csv\"\n",
    "\n",
    "# Đọc file CSV\n",
    "df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "# Tùy chọn: Giới hạn số dòng để xử lý (bỏ comment nếu muốn giới hạn)\n",
    "# df = df.head(4000).copy()\n",
    "print(f\"✓ Đã load {len(df)} dòng từ CSV\")\n",
    "print(f\"\\nCác cột trong file:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Kiểm tra cột target_text_vi\n",
    "if 'target_text_vi' in df.columns:\n",
    "    print(f\"\\n✓ Cột 'target_text_vi' có sẵn\")\n",
    "    print(f\"\\nVí dụ dòng đầu tiên:\")\n",
    "    print(df['target_text_vi'].iloc[0][:200])\n",
    "else:\n",
    "    print(f\"\\n⚠ Cột 'target_text_vi' không có trong file!\")\n",
    "    print(f\"Các cột có sẵn: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. In ra từng entity của mỗi sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã load 20454 samples\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Sample 1]\n",
      "Source text: Học sinh đánh giá của bạn được tìm thấy trên thiết bị có số serial IMEI: 20-612819-508665-8. Tài liệu này thuộc nhiều chủ đề trao đổi trong chúng ta Đ...\n",
      "Target text: Học sinh đánh giá của bạn được tìm thấy trên thiết bị có số serial IMEI: [PHONEIMEI]. Tài liệu này thuộc nhiều chủ đề trao đổi trong chúng ta [JOBAREA...\n",
      "\n",
      "Entities tìm thấy (2):\n",
      "  1. [PHONEIMEI] 20 - 612819 - 508665 - 8\n",
      "  2. [JOBAREA] Điện tử\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 2]\n",
      "Source text: Xin chào Linh, theo ghi chép của chúng tôi, giấy phép 4KHLBP0ZT3AX42032 của bạn vẫn đang được ghi nhận trong danh sách của chúng tôi để truy cập vào c...\n",
      "Target text: Xin chào [FIRSTNAME], theo ghi chép của chúng tôi, giấy phép [VEHICLEVIN] của bạn vẫn đang được ghi nhận trong danh sách của chúng tôi để truy cập vào...\n",
      "\n",
      "Entities tìm thấy (2):\n",
      "  1. [FIRSTNAME] Linh\n",
      "  2. [VEHICLEVIN] 4KHLBP0ZT3AX42032\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 3]\n",
      "Source text: Trí đáng sẽ có thể hỏi ý kiến về chế độ ăn chay cho người trong tuổi 90 nữ với chiều cao 1.52m không?...\n",
      "Target text: [FIRSTNAME] đáng sẽ có thể hỏi ý kiến về chế độ ăn chay cho người trong tuổi [AGE] [GENDER] với chiều cao [HEIGHT] không?...\n",
      "\n",
      "Entities tìm thấy (4):\n",
      "  1. [FIRSTNAME] Trí\n",
      "  2. [AGE] 90\n",
      "  3. [GENDER] nữ\n",
      "  4. [HEIGHT] 1. 52m\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 4]\n",
      "Source text: Dạn, các hàng hóa an  ninh trong số nhà số 708 cần được nhập lại. Sử dụng 7531663452431552 để thanh toán cho chúng....\n",
      "Target text: Dạn, các hàng hóa an  ninh trong số nhà số [BUILDINGNUMBER] cần được nhập lại. Sử dụng [MASKEDNUMBER] để thanh toán cho chúng....\n",
      "\n",
      "Entities tìm thấy (2):\n",
      "  1. [BUILDINGNUMBER] 708\n",
      "  2. [MASKEDNUMBER] 7531663452431552\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 5]\n",
      "Source text: Đến tuổi 53, trẻ em tại số nhà 96 đã hiển thị khả năng đáng chú ý khi nhớ và phát biểu mật khẩu, với tJ3cJmhbO25_ là mật khẩu được lặp lại nhiều nhất....\n",
      "Target text: Đến tuổi [AGE], trẻ em tại số nhà [BUILDINGNUMBER] đã hiển thị khả năng đáng chú ý khi nhớ và phát biểu mật khẩu, với [PASSWORD] là mật khẩu được lặp ...\n",
      "\n",
      "Entities tìm thấy (3):\n",
      "  1. [AGE] 53\n",
      "  2. [BUILDINGNUMBER] 96\n",
      "  3. [PASSWORD] tJ3cJmhbO25 _\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 6]\n",
      "Source text: Dữ liệu bạn mới được ghi lại tại bệnh viện về quản lý bệnh tụnh đã được mã hóa bằng IPv6 [IPV6] để tăng độ bảo mật riêng tư hơn....\n",
      "Target text: Dữ liệu bạn mới được ghi lại tại bệnh viện về quản lý bệnh tụnh đã được mã hóa bằng IPv6 [IPV6] để tăng độ bảo mật riêng tư hơn....\n",
      "\n",
      "  Không có entity nào\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 7]\n",
      "Source text: Quý nữ! Let's giải quét sự bất ngờ. Chúng ta không gửi thư điện tử yêu cầu bạn sZDILeWMrYfz. Nếu bạn nhận được một thư đó, nó không từ chúng ta. Hãy b...\n",
      "Target text: Quý [GENDER]! Let's giải quét sự bất ngờ. Chúng ta không gửi thư điện tử yêu cầu bạn [PASSWORD]. Nếu bạn nhận được một thư đó, nó không từ chúng ta. H...\n",
      "\n",
      "Entities tìm thấy (2):\n",
      "  1. [GENDER] nữ\n",
      "  2. [PASSWORD] sZDILeWMrYfz\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 8]\n",
      "Source text: Portal hoạt động tốt đến đường dẫn [69.4748, -62.3888]. Với hàng loạt các đường đi sức khỏe phù hợp cho mọi thành viên trong gia đình, đăng ký ngay bạ...\n",
      "Target text: Portal hoạt động tốt đến đường dẫn [NEARBYGPSCOORDINATE]. Với hàng loạt các đường đi sức khỏe phù hợp cho mọi thành viên trong gia đình, đăng ký ngay ...\n",
      "\n",
      "Entities tìm thấy (2):\n",
      "  1. [NEARBYGPSCOORDINATE] [ 69. 4748, - 62. 3888 ]\n",
      "  2. [USERAGENT] Mozilla / 5. 0 ( Macintosh ; PPC Mac OS X 10 _ 6 _ 3 ) AppleWebKit / 537. 2. 0 ( KHTML, like Gecko ) Chrome / 36. 0. 823. 0 Safari / 537. 2. 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 9]\n",
      "Source text: Nhung, các công cụ học tập mới tương tác sẽ đến đến vị trí bảng địa [6.614, -175.067]. Mật khẩu của bạn để truy cập nội dung trực tuyến: GgpdWbMKzEKD....\n",
      "Target text: [FIRSTNAME], các công cụ học tập mới tương tác sẽ đến đến vị trí bảng địa [NEARBYGPSCOORDINATE]. Mật khẩu của bạn để truy cập nội dung trực tuyến: [PA...\n",
      "\n",
      "Entities tìm thấy (3):\n",
      "  1. [FIRSTNAME] Nhung\n",
      "  2. [NEARBYGPSCOORDINATE] [ 6. 614, - 175. 067 ]\n",
      "  3. [PASSWORD] GgpdWbMKzEKD\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 10]\n",
      "Source text: 1. Thông tin yêu cầu khách hàng nhận được lại vào thời gian 18:58 từ chủ trương của doanh nghiệp tại quận Huyện Vĩnh Bảo. Địa chỉ email của khách hàng...\n",
      "Target text: 1. Thông tin yêu cầu khách hàng nhận được lại vào thời gian [TIME] từ chủ trương của doanh nghiệp tại quận [COUNTY]. Địa chỉ email của khách hàng là [...\n",
      "\n",
      "Entities tìm thấy (5):\n",
      "  1. [TIME] 18 : 58\n",
      "  2. [COUNTY] Huyện Vĩnh Bảo\n",
      "  3. [EMAIL] chaubui @ outlook. com\n",
      "  4. [PIN] 0080\n",
      "  5. [EYECOLOR] Đen\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "... (còn 20444 samples khác)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def extract_entities_from_labels(tokens, labels):\n",
    "    \"\"\"\n",
    "    Trích xuất các entity từ tokens và labels\n",
    "    \n",
    "    Args:\n",
    "        tokens: List tokens\n",
    "        labels: List labels theo BIO format\n",
    "    \n",
    "    Returns:\n",
    "        List các tuple (entity_text, entity_label)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label.startswith('B-'):\n",
    "            # Bắt đầu entity mới\n",
    "            if current_entity:\n",
    "                # Lưu entity trước đó\n",
    "                entity_text = tokenizer.convert_tokens_to_string(current_entity).strip()\n",
    "                entities.append((entity_text, current_label))\n",
    "            current_entity = [token]\n",
    "            current_label = label[2:]  # Bỏ 'B-' prefix\n",
    "        elif label.startswith('I-') and current_label == label[2:]:\n",
    "            # Tiếp tục entity hiện tại\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            # Kết thúc entity\n",
    "            if current_entity:\n",
    "                entity_text = tokenizer.convert_tokens_to_string(current_entity).strip()\n",
    "                entities.append((entity_text, current_label))\n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "    \n",
    "    # Lưu entity cuối cùng nếu có\n",
    "    if current_entity:\n",
    "        entity_text = tokenizer.convert_tokens_to_string(current_entity).strip()\n",
    "        entities.append((entity_text, current_label))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Load file đã xử lý\n",
    "df_loaded = pd.read_csv('data/demasked_and_tokenized.csv', encoding='utf-8')\n",
    "\n",
    "print(f\"Đã load {len(df_loaded)} samples\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# In ra từng entity của mỗi sample\n",
    "for idx in range(len(df_loaded)):\n",
    "    print(f\"\\n[Sample {idx + 1}]\")\n",
    "    print(f\"Source text: {df_loaded['source_text'].iloc[idx][:150]}...\")\n",
    "    print(f\"Target text: {df_loaded['target_text'].iloc[idx][:150]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Parse tokens và labels\n",
    "        tokens = ast.literal_eval(df_loaded['tokens'].iloc[idx])\n",
    "        labels = ast.literal_eval(df_loaded['labels'].iloc[idx])\n",
    "        \n",
    "        # Trích xuất entities\n",
    "        entities = extract_entities_from_labels(tokens, labels)\n",
    "        \n",
    "        if entities:\n",
    "            print(f\"\\nEntities tìm thấy ({len(entities)}):\")\n",
    "            for i, (entity_text, entity_label) in enumerate(entities, 1):\n",
    "                print(f\"  {i}. [{entity_label}] {entity_text}\")\n",
    "        else:\n",
    "            print(\"\\n  Không có entity nào\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ⚠ Lỗi khi parse: {e}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Chỉ in 10 samples đầu tiên để demo (có thể bỏ comment để in tất cả)\n",
    "    if idx >= 9:\n",
    "        print(f\"\\n... (còn {len(df_loaded) - 10} samples khác)\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. In tất cả entities hoặc lưu vào file (tùy chọn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Đã lưu 20454 samples vào: data/entities_by_sample.json\n",
      "✓ Đã lưu entities vào: data/entities_by_sample.csv\n",
      "  Tổng số dòng: 57120\n",
      "  Số samples có entity: 56623\n"
     ]
    }
   ],
   "source": [
    "# Tùy chọn: In tất cả samples hoặc lưu vào file\n",
    "PRINT_ALL = False  # Đặt True để in tất cả samples\n",
    "SAVE_TO_FILE = True  # Đặt True để lưu vào file\n",
    "\n",
    "if PRINT_ALL or SAVE_TO_FILE:\n",
    "    all_entities_data = []\n",
    "    \n",
    "    for idx in range(len(df_loaded)):\n",
    "        try:\n",
    "            tokens = ast.literal_eval(df_loaded['tokens'].iloc[idx])\n",
    "            labels = ast.literal_eval(df_loaded['labels'].iloc[idx])\n",
    "            entities = extract_entities_from_labels(tokens, labels)\n",
    "            \n",
    "            sample_data = {\n",
    "                'sample_id': idx + 1,\n",
    "                'source_text': df_loaded['source_text'].iloc[idx],\n",
    "                'target_text': df_loaded['target_text'].iloc[idx],\n",
    "                'num_entities': len(entities),\n",
    "                'entities': entities\n",
    "            }\n",
    "            all_entities_data.append(sample_data)\n",
    "            \n",
    "            if PRINT_ALL:\n",
    "                print(f\"\\n[Sample {idx + 1}]\")\n",
    "                print(f\"Source: {df_loaded['source_text'].iloc[idx][:100]}...\")\n",
    "                if entities:\n",
    "                    print(f\"Entities ({len(entities)}):\")\n",
    "                    for i, (entity_text, entity_label) in enumerate(entities, 1):\n",
    "                        print(f\"  {i}. [{entity_label}] {entity_text}\")\n",
    "                else:\n",
    "                    print(\"  Không có entity\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "        except Exception as e:\n",
    "            if PRINT_ALL:\n",
    "                print(f\"\\n[Sample {idx + 1}] ⚠ Lỗi: {e}\")\n",
    "    \n",
    "    if SAVE_TO_FILE:\n",
    "        # Lưu dưới dạng JSON\n",
    "        import json\n",
    "        output_json = 'data/entities_by_sample.json'\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_entities_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\n✓ Đã lưu {len(all_entities_data)} samples vào: {output_json}\")\n",
    "        \n",
    "        # Lưu dưới dạng CSV (flattened)\n",
    "        entities_rows = []\n",
    "        for sample in all_entities_data:\n",
    "            if sample['entities']:\n",
    "                for entity_text, entity_label in sample['entities']:\n",
    "                    entities_rows.append({\n",
    "                        'sample_id': sample['sample_id'],\n",
    "                        'entity_label': entity_label,\n",
    "                        'entity_text': entity_text,\n",
    "                        'source_text': sample['source_text'][:200]  # Truncate để dễ đọc\n",
    "                    })\n",
    "            else:\n",
    "                entities_rows.append({\n",
    "                    'sample_id': sample['sample_id'],\n",
    "                    'entity_label': None,\n",
    "                    'entity_text': None,\n",
    "                    'source_text': sample['source_text'][:200]\n",
    "                })\n",
    "        \n",
    "        df_entities = pd.DataFrame(entities_rows)\n",
    "        output_csv = 'data/entities_by_sample.csv'\n",
    "        df_entities.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "        print(f\"✓ Đã lưu entities vào: {output_csv}\")\n",
    "        print(f\"  Tổng số dòng: {len(df_entities)}\")\n",
    "        print(f\"  Số samples có entity: {df_entities['entity_label'].notna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hàm Demask và Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demask_and_tokenize(text, pii_data, tokenizer):\n",
    "    \"\"\"\n",
    "    Demask text và tokenize với mBERT, đồng thời gán labels\n",
    "    \n",
    "    Args:\n",
    "        text: Text có chứa PII placeholders như [LABEL] hoặc <LABEL>value</LABEL>\n",
    "        pii_data: Dictionary chứa các danh sách PII theo label\n",
    "        tokenizer: mBERT tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        source_text: Text đã demask (không có XML tags)\n",
    "        target_text: Text gốc với placeholders\n",
    "        tokens: List tokens từ mBERT\n",
    "        labels: List labels theo BIO format\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return '', '', [], []\n",
    "    \n",
    "    text = str(text)\n",
    "    target_text = text  # Giữ nguyên text gốc\n",
    "    \n",
    "    # Bước 1: Demask - Thay [LABEL] bằng <LABEL>value</LABEL>\n",
    "    pattern1 = r'\\[([A-Z_0-9]+)\\]'  # Thêm 0-9 để match IPV4, IPV6, v.v.\n",
    "    def replace_label1(match):\n",
    "        label = match.group(1)\n",
    "        if label in pii_data and len(pii_data[label]) > 0:\n",
    "            return f\"<{label}>{random.choice(pii_data[label])}</{label}>\"\n",
    "        return match.group(0)  # Giữ nguyên nếu không tìm thấy\n",
    "    \n",
    "    text_with_xml = re.sub(pattern1, replace_label1, text)\n",
    "    \n",
    "    # Bước 2: Loại bỏ XML tags để tạo source_text (chỉ giữ giá trị)\n",
    "    pattern_xml = r'<([A-Z_0-9]+)>(.*?)</\\1>'  # Thêm 0-9 để match IPV4, IPV6, v.v.\n",
    "    source_text = re.sub(pattern_xml, r'\\2', text_with_xml)\n",
    "    \n",
    "    # Bước 3: Tokenize và gán labels dựa trên XML tags\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    # Parse text với XML tags để tách entity và non-entity\n",
    "    segments = []\n",
    "    last_pos = 0\n",
    "    \n",
    "    for match in re.finditer(pattern_xml, text_with_xml):\n",
    "        # Text trước entity\n",
    "        before_text = text_with_xml[last_pos:match.start()].strip()\n",
    "        if before_text:\n",
    "            segments.append((before_text, None))\n",
    "        \n",
    "        # Entity text\n",
    "        label = match.group(1)\n",
    "        entity_text = match.group(2).strip()\n",
    "        if entity_text:\n",
    "            segments.append((entity_text, label))\n",
    "        \n",
    "        last_pos = match.end()\n",
    "    \n",
    "    # Text sau entity cuối cùng\n",
    "    after_text = text_with_xml[last_pos:].strip()\n",
    "    if after_text:\n",
    "        segments.append((after_text, None))\n",
    "    \n",
    "    # Nếu không có XML tags, tokenize toàn bộ text\n",
    "    if not segments:\n",
    "        segments = [(text_with_xml, None)]\n",
    "    \n",
    "    # Tokenize từng segment và gán labels\n",
    "    # Track previous label để tránh 2 B- liên tiếp cho cùng entity\n",
    "    prev_label = None\n",
    "    \n",
    "    for segment_text, entity_label in segments:\n",
    "        segment_tokens = tokenizer.tokenize(segment_text)\n",
    "        \n",
    "        if entity_label:\n",
    "            # Entity segment: B-label chỉ cho token đầu tiên, I-label cho tất cả token còn lại\n",
    "            for i, token in enumerate(segment_tokens):\n",
    "                tokens.append(token)\n",
    "                \n",
    "                if i == 0:\n",
    "                    # Token đầu tiên của entity: B-label chỉ khi token trước không phải I- hoặc B- của cùng entity\n",
    "                    if prev_label == f\"I-{entity_label}\" or prev_label == f\"B-{entity_label}\":\n",
    "                        # Nếu token trước là I- hoặc B- của cùng entity, thì token này cũng là I-\n",
    "                        # (tránh 2 B- liên tiếp cho cùng entity)\n",
    "                        labels.append(f\"I-{entity_label}\")\n",
    "                        prev_label = f\"I-{entity_label}\"\n",
    "                    else:\n",
    "                        # Token đầu tiên của entity mới: B-label\n",
    "                        labels.append(f\"B-{entity_label}\")\n",
    "                        prev_label = f\"B-{entity_label}\"\n",
    "                else:\n",
    "                    # Tất cả token còn lại: I-label (kể cả các word không phải subword)\n",
    "                    labels.append(f\"I-{entity_label}\")\n",
    "                    prev_label = f\"I-{entity_label}\"\n",
    "        else:\n",
    "            # Non-entity segment: O labels\n",
    "            for token in segment_tokens:\n",
    "                tokens.append(token)\n",
    "                labels.append('O')\n",
    "                prev_label = 'O'\n",
    "    \n",
    "    return source_text, target_text, tokens, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test hàm với ví dụ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'target_text_vi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADMIN\\miniconda3\\envs\\pii\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target_text_vi'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test với một ví dụ\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m     test_text \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget_text_vi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText gốc:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\miniconda3\\envs\\pii\\lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\miniconda3\\envs\\pii\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target_text_vi'"
     ]
    }
   ],
   "source": [
    "# Test với một ví dụ\n",
    "if len(df) > 0:\n",
    "    test_text = df['target_text_vi'].iloc[0]\n",
    "    print(f\"Text gốc:\")\n",
    "    print(f\"  {test_text}\")\n",
    "    print()\n",
    "    \n",
    "    source, target, tokens, labels = demask_and_tokenize(test_text, pii_data, tokenizer)\n",
    "    \n",
    "    print(f\"Source text (đã demask, không có XML tags):\")\n",
    "    print(f\"  {source[:200]}...\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Tokens ({len(tokens)} tokens):\")\n",
    "    print(f\"  {tokens[:]}...\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Labels ({len(labels)} labels):\")\n",
    "    print(f\"  {labels[:]}...\")\n",
    "    print()\n",
    "    \n",
    "    # Đếm entities\n",
    "    entity_count = len([l for l in labels if l != 'O'])\n",
    "    unique_entities = set([l.split('-')[1] for l in labels if '-' in l])\n",
    "    print(f\"Entities tìm thấy: {sorted(unique_entities)}\")\n",
    "    print(f\"Số entity tokens: {entity_count}/{len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Áp dụng cho toàn bộ dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xử lý 24296 dòng...\n",
      "  Đã xử lý 1000/24296 dòng...\n",
      "  Đã xử lý 2000/24296 dòng...\n",
      "  Đã xử lý 3000/24296 dòng...\n",
      "  Đã xử lý 4000/24296 dòng...\n",
      "  Đã xử lý 5000/24296 dòng...\n",
      "  Đã xử lý 6000/24296 dòng...\n",
      "  Đã xử lý 7000/24296 dòng...\n",
      "  Đã xử lý 8000/24296 dòng...\n",
      "  Đã xử lý 9000/24296 dòng...\n",
      "  Đã xử lý 10000/24296 dòng...\n",
      "  Đã xử lý 11000/24296 dòng...\n",
      "  Đã xử lý 12000/24296 dòng...\n",
      "  Đã xử lý 13000/24296 dòng...\n",
      "  Đã xử lý 14000/24296 dòng...\n",
      "  Đã xử lý 15000/24296 dòng...\n",
      "  Đã xử lý 16000/24296 dòng...\n",
      "  Đã xử lý 17000/24296 dòng...\n",
      "  Đã xử lý 18000/24296 dòng...\n",
      "  Đã xử lý 19000/24296 dòng...\n",
      "  Đã xử lý 20000/24296 dòng...\n",
      "  Đã xử lý 21000/24296 dòng...\n",
      "  Đã xử lý 22000/24296 dòng...\n",
      "  Đã xử lý 23000/24296 dòng...\n",
      "  Đã xử lý 24000/24296 dòng...\n",
      "\n",
      "✓ Hoàn thành! Đã xử lý 24296 dòng\n"
     ]
    }
   ],
   "source": [
    "# Tạo các cột mới\n",
    "df['source_text'] = ''\n",
    "df['target_text'] = df['target_text_vi']\n",
    "df['tokens'] = None\n",
    "df['labels'] = None\n",
    "\n",
    "print(f\"Đang xử lý {len(df)} dòng...\")\n",
    "errors = 0\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    if (idx + 1) % 1000 == 0:\n",
    "        print(f\"  Đã xử lý {idx + 1}/{len(df)} dòng...\")\n",
    "    \n",
    "    try:\n",
    "        text = df.loc[idx, 'target_text_vi']\n",
    "        source, target, tokens, labels = demask_and_tokenize(text, pii_data, tokenizer)\n",
    "        \n",
    "        df.loc[idx, 'source_text'] = source\n",
    "        df.loc[idx, 'target_text'] = target\n",
    "        df.loc[idx, 'tokens'] = str(tokens)  # Lưu dưới dạng string\n",
    "        df.loc[idx, 'labels'] = str(labels)  # Lưu dưới dạng string\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        if errors <= 5:\n",
    "            print(f\"  ⚠ Lỗi ở dòng {idx}: {e}\")\n",
    "        df.loc[idx, 'source_text'] = ''\n",
    "        df.loc[idx, 'target_text'] = text\n",
    "        df.loc[idx, 'tokens'] = str([])\n",
    "        df.loc[idx, 'labels'] = str([])\n",
    "\n",
    "print(f\"\\n✓ Hoàn thành! Đã xử lý {len(df)} dòng\")\n",
    "if errors > 0:\n",
    "    print(f\"  ⚠ Có {errors} lỗi (đã bỏ qua)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Chỉ giữ lại 4 cột cần thiết\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataFrame cuối cùng có 24296 dòng và 4 cột\n",
      "\n",
      "Các cột:\n",
      "  - source_text\n",
      "  - target_text\n",
      "  - tokens\n",
      "  - labels\n",
      "\n",
      "Kiểm tra dữ liệu:\n",
      "  - Dòng có source_text: 24296/24296\n",
      "  - Dòng có target_text: 24296/24296\n",
      "  - Dòng có tokens: 24296/24296\n",
      "  - Dòng có labels: 24296/24296\n",
      "\n",
      "=== VÍ DỤ 3 DÒNG ĐẦU ===\n",
      "\n",
      "Dòng 0:\n",
      "  source_text: Cơ sở dữ liệu tiêm chủng nhân viên sẽ được duy trì bởi Thư ký. Tất cả các mục sẽ được xác minh bằng ...\n",
      "  target_text: Cơ sở dữ liệu tiêm chủng nhân viên sẽ được duy trì bởi [JOBAREA]. Tất cả các mục sẽ được xác minh bằ...\n",
      "  tokens: 62 tokens\n",
      "  labels: 62 labels\n",
      "  entities: 14 tokens\n",
      "\n",
      "Dòng 1:\n",
      "  source_text: Chào Ronny.Emard, buổi tư vấn chấn thương của bạn được lên lịch vào ngày 20.10.1999. Vui lòng đến cơ...\n",
      "  target_text: Chào [USERNAME], buổi tư vấn chấn thương của bạn được lên lịch vào ngày [DATE]. Vui lòng đến cơ sở c...\n",
      "  tokens: 40 tokens\n",
      "  labels: 40 labels\n",
      "  entities: 12 tokens\n",
      "\n",
      "Dòng 2:\n",
      "  source_text: Chào Hân, công ty chúng tôi đang lên kế hoạch gửi đi một dòng sản phẩm thiết bị y tế mới. Địa điểm c...\n",
      "  target_text: Chào [FIRSTNAME], công ty chúng tôi đang lên kế hoạch gửi đi một dòng sản phẩm thiết bị y tế mới. Đị...\n",
      "  tokens: 77 tokens\n",
      "  labels: 77 labels\n",
      "  entities: 21 tokens\n"
     ]
    }
   ],
   "source": [
    "# Chỉ giữ lại 4 cột: source_text, target_text, tokens, labels\n",
    "df_final = df[['source_text', 'target_text', 'tokens', 'labels']].copy()\n",
    "\n",
    "print(f\"✓ DataFrame cuối cùng có {len(df_final)} dòng và {len(df_final.columns)} cột\")\n",
    "print(f\"\\nCác cột:\")\n",
    "for col in df_final.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Kiểm tra dữ liệu\n",
    "print(f\"\\nKiểm tra dữ liệu:\")\n",
    "print(f\"  - Dòng có source_text: {df_final['source_text'].notna().sum()}/{len(df_final)}\")\n",
    "print(f\"  - Dòng có target_text: {df_final['target_text'].notna().sum()}/{len(df_final)}\")\n",
    "print(f\"  - Dòng có tokens: {df_final['tokens'].notna().sum()}/{len(df_final)}\")\n",
    "print(f\"  - Dòng có labels: {df_final['labels'].notna().sum()}/{len(df_final)}\")\n",
    "\n",
    "# Hiển thị ví dụ\n",
    "print(f\"\\n=== VÍ DỤ 3 DÒNG ĐẦU ===\")\n",
    "for i in range(min(3, len(df_final))):\n",
    "    print(f\"\\nDòng {i}:\")\n",
    "    print(f\"  source_text: {df_final['source_text'].iloc[i][:100]}...\")\n",
    "    print(f\"  target_text: {df_final['target_text'].iloc[i][:100]}...\")\n",
    "    \n",
    "    import ast\n",
    "    try:\n",
    "        tokens = ast.literal_eval(df_final['tokens'].iloc[i])\n",
    "        labels = ast.literal_eval(df_final['labels'].iloc[i])\n",
    "        print(f\"  tokens: {len(tokens)} tokens\")\n",
    "        print(f\"  labels: {len(labels)} labels\")\n",
    "        entity_count = len([l for l in labels if l != 'O'])\n",
    "        print(f\"  entities: {entity_count} tokens\")\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Lưu file kết quả\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Đã lưu file: data/dong10000_32000_d.csv\n",
      "  Số dòng: 24296\n",
      "  Số cột: 4\n",
      "\n",
      "Cấu trúc file:\n",
      "  - source_text: Text đã demask (không có XML tags)\n",
      "  - target_text: Text gốc với PII placeholders\n",
      "  - tokens: Danh sách tokens từ mBERT (dạng string)\n",
      "  - labels: Danh sách labels theo BIO format (dạng string)\n"
     ]
    }
   ],
   "source": [
    "# Lưu file CSV cuối cùng\n",
    "output_file = 'data/dong10000_32000_d.csv'\n",
    "\n",
    "\n",
    "df_final.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"✓ Đã lưu file: {output_file}\")\n",
    "print(f\"  Số dòng: {len(df_final)}\")\n",
    "print(f\"  Số cột: {len(df_final.columns)}\")\n",
    "print(f\"\\nCấu trúc file:\")\n",
    "print(f\"  - source_text: Text đã demask (không có XML tags)\")\n",
    "print(f\"  - target_text: Text gốc với PII placeholders\")\n",
    "print(f\"  - tokens: Danh sách tokens từ mBERT (dạng string)\")\n",
    "print(f\"  - labels: Danh sách labels theo BIO format (dạng string)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
